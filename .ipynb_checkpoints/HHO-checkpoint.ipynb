{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "528784ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4         5         6  \\\n",
      "0  0.074084 -0.277627  0.224151  0.076006 -0.101025 -0.222507 -0.419053   \n",
      "1  0.060406 -0.379488  0.305922  0.050496 -0.094420 -0.232423 -0.231077   \n",
      "2  0.073991 -0.415058  0.323547  0.140622 -0.077287 -0.263134 -0.251312   \n",
      "3  0.009951 -0.392846  0.284209  0.145580 -0.095777 -0.218694 -0.278221   \n",
      "4  0.107767 -0.442587  0.336645  0.118744 -0.130581 -0.333466 -0.382193   \n",
      "\n",
      "          7         8         9  ...       311       312       313       314  \\\n",
      "0 -0.032713 -0.315479 -0.255169  ... -0.046795  0.324696 -0.003093 -0.188499   \n",
      "1 -0.026665 -0.189860 -0.293975  ...  0.022416  0.214915  0.043591 -0.063989   \n",
      "2 -0.016233 -0.196971 -0.334934  ...  0.025894  0.235436  0.094447 -0.037679   \n",
      "3 -0.071526 -0.299126 -0.275551  ... -0.118786  0.301045  0.092333 -0.135528   \n",
      "4 -0.031051 -0.246826 -0.259484  ... -0.105815  0.138109  0.051308 -0.193587   \n",
      "\n",
      "        315       316       317       318       319    y  \n",
      "0  0.085960 -0.078037  0.195555 -0.052625  0.007361  0.0  \n",
      "1  0.135018  0.001986  0.099131  0.118025  0.036410  0.0  \n",
      "2  0.218425  0.027690  0.100435  0.114489  0.074000  0.0  \n",
      "3  0.248948  0.034367  0.158722  0.065228  0.089062  0.0  \n",
      "4  0.189984 -0.037317  0.145659  0.061080 -0.006339  0.0  \n",
      "\n",
      "[5 rows x 321 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 130 entries, 0 to 129\n",
      "Columns: 321 entries, 0 to y\n",
      "dtypes: float64(321)\n",
      "memory usage: 326.1 KB\n",
      "None\n",
      "                0           1           2           3           4           5  \\\n",
      "count  130.000000  130.000000  130.000000  130.000000  130.000000  130.000000   \n",
      "mean     0.082307   -0.387939    0.293743    0.254456   -0.034424   -0.231173   \n",
      "std      0.054473    0.101447    0.092298    0.102833    0.100029    0.116935   \n",
      "min     -0.051652   -0.669658    0.053330    0.034706   -0.266455   -0.538305   \n",
      "25%      0.049143   -0.450463    0.238282    0.182186   -0.095438   -0.314698   \n",
      "50%      0.085665   -0.394679    0.285844    0.252452   -0.037757   -0.238604   \n",
      "75%      0.124190   -0.316323    0.351498    0.333124    0.037891   -0.135455   \n",
      "max      0.207926   -0.156004    0.611936    0.526040    0.196874    0.073945   \n",
      "\n",
      "                6           7           8           9  ...         311  \\\n",
      "count  130.000000  130.000000  130.000000  130.000000  ...  130.000000   \n",
      "mean    -0.328831   -0.067466   -0.181872   -0.214041  ...   -0.023761   \n",
      "std      0.096116    0.106682    0.093435    0.072387  ...    0.077695   \n",
      "min     -0.553824   -0.319365   -0.383233   -0.367533  ...   -0.220014   \n",
      "25%     -0.392610   -0.150253   -0.251765   -0.264008  ...   -0.077506   \n",
      "50%     -0.322019   -0.066672   -0.179507   -0.218446  ...   -0.026671   \n",
      "75%     -0.268865    0.004509   -0.116810   -0.165474  ...    0.027457   \n",
      "max     -0.074141    0.190879    0.091021   -0.033976  ...    0.147404   \n",
      "\n",
      "              312         313         314         315         316         317  \\\n",
      "count  130.000000  130.000000  130.000000  130.000000  130.000000  130.000000   \n",
      "mean     0.127882    0.029152   -0.022295    0.196615    0.086791    0.123031   \n",
      "std      0.112311    0.094971    0.095474    0.075427    0.120087    0.096150   \n",
      "min     -0.130157   -0.194359   -0.215261   -0.015516   -0.166370   -0.130484   \n",
      "25%      0.055451   -0.036238   -0.088505    0.155003    0.000748    0.060555   \n",
      "50%      0.125612    0.023561   -0.026697    0.194289    0.083392    0.144644   \n",
      "75%      0.205218    0.091158    0.039777    0.250616    0.161751    0.189916   \n",
      "max      0.366453    0.306630    0.200978    0.339524    0.388206    0.418161   \n",
      "\n",
      "              318         319           y  \n",
      "count  130.000000  130.000000  130.000000  \n",
      "mean     0.161746    0.032421    0.991069  \n",
      "std      0.123460    0.089557    1.456959  \n",
      "min     -0.128202   -0.202852    0.000000  \n",
      "25%      0.065304   -0.018103    0.178500  \n",
      "50%      0.172034    0.038765    0.496500  \n",
      "75%      0.249109    0.093475    0.877250  \n",
      "max      0.430007    0.220815    6.169000  \n",
      "\n",
      "[8 rows x 321 columns]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************查看训练集的形状**************************\n",
      "(104, 320, 1)\n",
      "***********************查看测试集的形状**************************\n",
      "(26, 320, 1)\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 141ms/step - loss: 2.9281 - mse: 2.9281 - val_loss: 1.8272 - val_mse: 1.8272\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.2415 - mse: 2.2415 - val_loss: 1.8496 - val_mse: 1.8496\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.2709 - mse: 2.2709 - val_loss: 1.8096 - val_mse: 1.8096\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.1991 - mse: 2.1991 - val_loss: 1.8329 - val_mse: 1.8329\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1733 - mse: 2.1733 - val_loss: 1.8235 - val_mse: 1.8235\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1752 - mse: 2.1752 - val_loss: 1.7742 - val_mse: 1.7742\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 2.1567 - mse: 2.1567 - val_loss: 1.7702 - val_mse: 1.7702\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1003 - mse: 2.1003 - val_loss: 1.6918 - val_mse: 1.6918\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.0189 - mse: 2.0189 - val_loss: 1.5437 - val_mse: 1.5437\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 1.8792 - mse: 1.8792 - val_loss: 1.5340 - val_mse: 1.5340\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 181ms/step - loss: 2.9669 - mse: 2.9669 - val_loss: 1.8583 - val_mse: 1.8583\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.2956 - mse: 2.2956 - val_loss: 1.8442 - val_mse: 1.8442\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.2130 - mse: 2.2130 - val_loss: 1.8360 - val_mse: 1.8360\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1771 - mse: 2.1771 - val_loss: 1.8190 - val_mse: 1.8190\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1751 - mse: 2.1751 - val_loss: 1.8093 - val_mse: 1.8093\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1719 - mse: 2.1719 - val_loss: 1.8159 - val_mse: 1.8159\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1853 - mse: 2.1853 - val_loss: 1.8127 - val_mse: 1.8127\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1421 - mse: 2.1421 - val_loss: 1.7526 - val_mse: 1.7526\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1340 - mse: 2.1340 - val_loss: 1.7217 - val_mse: 1.7217\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1686 - mse: 2.1686 - val_loss: 1.6907 - val_mse: 1.6907\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 240ms/step - loss: 2.9890 - mse: 2.9890 - val_loss: 1.8330 - val_mse: 1.8330\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.3608 - mse: 2.3608 - val_loss: 1.8295 - val_mse: 1.8295\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.3519 - mse: 2.3519 - val_loss: 1.8593 - val_mse: 1.8593\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1956 - mse: 2.1956 - val_loss: 1.8152 - val_mse: 1.8152\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.2452 - mse: 2.2452 - val_loss: 1.9982 - val_mse: 1.9982\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.1489 - mse: 2.1489 - val_loss: 1.7662 - val_mse: 1.7662\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 2.1848 - mse: 2.1848 - val_loss: 1.7624 - val_mse: 1.7624\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1556 - mse: 2.1556 - val_loss: 1.7466 - val_mse: 1.7466\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.0763 - mse: 2.0763 - val_loss: 1.7042 - val_mse: 1.7042\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.0162 - mse: 2.0162 - val_loss: 1.5641 - val_mse: 1.5641\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 163ms/step - loss: 2.7509 - mse: 2.7509 - val_loss: 1.8868 - val_mse: 1.8868\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.3283 - mse: 2.3283 - val_loss: 1.8173 - val_mse: 1.8173\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.2914 - mse: 2.2914 - val_loss: 1.8470 - val_mse: 1.8470\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2242 - mse: 2.2242 - val_loss: 1.8012 - val_mse: 1.8012\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1516 - mse: 2.1516 - val_loss: 1.8366 - val_mse: 1.8366\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1550 - mse: 2.1550 - val_loss: 1.8055 - val_mse: 1.8055\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 2.1170 - mse: 2.1170 - val_loss: 1.7399 - val_mse: 1.7399\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.0989 - mse: 2.0989 - val_loss: 1.6978 - val_mse: 1.6978\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.0593 - mse: 2.0593 - val_loss: 1.6067 - val_mse: 1.6067\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.0394 - mse: 2.0394 - val_loss: 1.5061 - val_mse: 1.5061\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 250ms/step - loss: 2.9373 - mse: 2.9373 - val_loss: 1.8909 - val_mse: 1.8909\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.3333 - mse: 2.3333 - val_loss: 1.8391 - val_mse: 1.8391\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.2039 - mse: 2.2039 - val_loss: 1.8468 - val_mse: 1.8468\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1853 - mse: 2.1853 - val_loss: 1.8040 - val_mse: 1.8040\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1566 - mse: 2.1566 - val_loss: 1.7637 - val_mse: 1.7637\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1077 - mse: 2.1077 - val_loss: 1.7240 - val_mse: 1.7240\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.0416 - mse: 2.0416 - val_loss: 1.6601 - val_mse: 1.6601\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 1.9465 - mse: 1.9465 - val_loss: 1.4511 - val_mse: 1.4511\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 1.8298 - mse: 1.8298 - val_loss: 7.2812 - val_mse: 7.2812\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 1s 74ms/step - loss: 5.2280 - mse: 5.2280 - val_loss: 3.6575 - val_mse: 3.6575\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 171ms/step - loss: 2.9818 - mse: 2.9818 - val_loss: 1.9013 - val_mse: 1.9013\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.4728 - mse: 2.4728 - val_loss: 1.8542 - val_mse: 1.8542\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1837 - mse: 2.1837 - val_loss: 1.8401 - val_mse: 1.8401\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2353 - mse: 2.2353 - val_loss: 1.8142 - val_mse: 1.8142\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.2204 - mse: 2.2204 - val_loss: 1.9111 - val_mse: 1.9111\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1959 - mse: 2.1959 - val_loss: 1.8690 - val_mse: 1.8690\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1844 - mse: 2.1844 - val_loss: 1.8251 - val_mse: 1.8251\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2097 - mse: 2.2097 - val_loss: 1.8139 - val_mse: 1.8139\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1818 - mse: 2.1818 - val_loss: 1.8213 - val_mse: 1.8213\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1633 - mse: 2.1633 - val_loss: 1.8312 - val_mse: 1.8312\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 162ms/step - loss: 3.0229 - mse: 3.0229 - val_loss: 1.8822 - val_mse: 1.8822\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2354 - mse: 2.2354 - val_loss: 1.8264 - val_mse: 1.8264\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2227 - mse: 2.2227 - val_loss: 1.8116 - val_mse: 1.8116\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1902 - mse: 2.1902 - val_loss: 1.8739 - val_mse: 1.8739\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.2044 - mse: 2.2044 - val_loss: 1.8013 - val_mse: 1.8013\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1939 - mse: 2.1939 - val_loss: 1.8408 - val_mse: 1.8408\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1736 - mse: 2.1736 - val_loss: 1.7574 - val_mse: 1.7574\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1545 - mse: 2.1545 - val_loss: 1.7779 - val_mse: 1.7779\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1362 - mse: 2.1362 - val_loss: 1.6946 - val_mse: 1.6946\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.0465 - mse: 2.0465 - val_loss: 1.6010 - val_mse: 1.6010\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 161ms/step - loss: 2.8622 - mse: 2.8622 - val_loss: 1.8686 - val_mse: 1.8686\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.5329 - mse: 2.5329 - val_loss: 1.9102 - val_mse: 1.9102\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1806 - mse: 2.1806 - val_loss: 1.8416 - val_mse: 1.8416\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2537 - mse: 2.2537 - val_loss: 1.8205 - val_mse: 1.8205\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1954 - mse: 2.1954 - val_loss: 1.8373 - val_mse: 1.8373\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.2114 - mse: 2.2114 - val_loss: 1.9071 - val_mse: 1.9071\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1833 - mse: 2.1833 - val_loss: 1.8490 - val_mse: 1.8490\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1690 - mse: 2.1690 - val_loss: 1.8153 - val_mse: 1.8153\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1875 - mse: 2.1875 - val_loss: 1.8068 - val_mse: 1.8068\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1757 - mse: 2.1757 - val_loss: 1.8085 - val_mse: 1.8085\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 158ms/step - loss: 2.9115 - mse: 2.9115 - val_loss: 1.8537 - val_mse: 1.8537\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.2721 - mse: 2.2721 - val_loss: 1.8262 - val_mse: 1.8262\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1920 - mse: 2.1920 - val_loss: 1.8427 - val_mse: 1.8427\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1645 - mse: 2.1645 - val_loss: 1.8199 - val_mse: 1.8199\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1379 - mse: 2.1379 - val_loss: 1.7545 - val_mse: 1.7545\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1775 - mse: 2.1775 - val_loss: 1.7185 - val_mse: 1.7185\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1115 - mse: 2.1115 - val_loss: 1.7139 - val_mse: 1.7139\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.0621 - mse: 2.0621 - val_loss: 1.5927 - val_mse: 1.5927\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 1.9550 - mse: 1.9550 - val_loss: 1.4564 - val_mse: 1.4564\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 73ms/step - loss: 1.8759 - mse: 1.8759 - val_loss: 1.4779 - val_mse: 1.4779\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 156ms/step - loss: 2.9013 - mse: 2.9013 - val_loss: 1.8572 - val_mse: 1.8572\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.2509 - mse: 2.2509 - val_loss: 1.8342 - val_mse: 1.8342\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1703 - mse: 2.1703 - val_loss: 1.8028 - val_mse: 1.8028\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1487 - mse: 2.1487 - val_loss: 1.7461 - val_mse: 1.7461\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1447 - mse: 2.1447 - val_loss: 1.7027 - val_mse: 1.7027\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1072 - mse: 2.1072 - val_loss: 1.6405 - val_mse: 1.6405\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.0779 - mse: 2.0779 - val_loss: 1.5540 - val_mse: 1.5540\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.9964 - mse: 1.9964 - val_loss: 1.4657 - val_mse: 1.4657\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.0950 - mse: 2.0950 - val_loss: 1.4433 - val_mse: 1.4433\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 1.8260 - mse: 1.8260 - val_loss: 1.4205 - val_mse: 1.4205\n",
      "********************************* 当前迭代次数:  1 ***************************************\n",
      "最好的适应度数值:  0.779899\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 164ms/step - loss: 2.9130 - mse: 2.9130 - val_loss: 1.8187 - val_mse: 1.8187\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.2164 - mse: 2.2164 - val_loss: 1.8060 - val_mse: 1.8060\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.1921 - mse: 2.1921 - val_loss: 1.7889 - val_mse: 1.7889\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2037 - mse: 2.2037 - val_loss: 1.8458 - val_mse: 1.8458\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.1322 - mse: 2.1322 - val_loss: 1.7415 - val_mse: 1.7415\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1413 - mse: 2.1413 - val_loss: 1.6973 - val_mse: 1.6973\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.0884 - mse: 2.0884 - val_loss: 1.6278 - val_mse: 1.6278\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 1.9701 - mse: 1.9701 - val_loss: 1.6643 - val_mse: 1.6643\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.0241 - mse: 2.0241 - val_loss: 1.4632 - val_mse: 1.4632\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.9641 - mse: 1.9641 - val_loss: 1.4546 - val_mse: 1.4546\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 166ms/step - loss: 2.9678 - mse: 2.9678 - val_loss: 1.8243 - val_mse: 1.8243\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.2279 - mse: 2.2279 - val_loss: 1.8182 - val_mse: 1.8182\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.2123 - mse: 2.2123 - val_loss: 1.8172 - val_mse: 1.8172\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.1494 - mse: 2.1494 - val_loss: 1.7789 - val_mse: 1.7789\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1603 - mse: 2.1603 - val_loss: 1.7882 - val_mse: 1.7882\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1293 - mse: 2.1293 - val_loss: 1.7261 - val_mse: 1.7261\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.2191 - mse: 2.2191 - val_loss: 1.6942 - val_mse: 1.6942\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1525 - mse: 2.1525 - val_loss: 1.8028 - val_mse: 1.8028\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.0191 - mse: 2.0191 - val_loss: 1.6006 - val_mse: 1.6006\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.9862 - mse: 1.9862 - val_loss: 1.5703 - val_mse: 1.5703\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 174ms/step - loss: 2.9203 - mse: 2.9203 - val_loss: 1.8501 - val_mse: 1.8501\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.3884 - mse: 2.3884 - val_loss: 1.8520 - val_mse: 1.8520\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2680 - mse: 2.2680 - val_loss: 1.8222 - val_mse: 1.8222\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.1856 - mse: 2.1856 - val_loss: 1.8895 - val_mse: 1.8895\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 2.2187 - mse: 2.2187 - val_loss: 1.8103 - val_mse: 1.8103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1591 - mse: 2.1591 - val_loss: 1.7754 - val_mse: 1.7754\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.1423 - mse: 2.1423 - val_loss: 1.7953 - val_mse: 1.7953\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1294 - mse: 2.1294 - val_loss: 1.7451 - val_mse: 1.7451\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.0759 - mse: 2.0759 - val_loss: 1.6593 - val_mse: 1.6593\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.0423 - mse: 2.0423 - val_loss: 1.5769 - val_mse: 1.5769\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 174ms/step - loss: 3.0428 - mse: 3.0428 - val_loss: 1.9470 - val_mse: 1.9470\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2748 - mse: 2.2748 - val_loss: 1.9359 - val_mse: 1.9359\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1939 - mse: 2.1939 - val_loss: 1.8203 - val_mse: 1.8203\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1923 - mse: 2.1923 - val_loss: 1.8217 - val_mse: 1.8217\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1874 - mse: 2.1874 - val_loss: 1.8380 - val_mse: 1.8380\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1714 - mse: 2.1714 - val_loss: 1.8182 - val_mse: 1.8182\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 1s 79ms/step - loss: 2.1822 - mse: 2.1822 - val_loss: 1.8168 - val_mse: 1.8168\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 1s 90ms/step - loss: 2.1730 - mse: 2.1730 - val_loss: 1.7977 - val_mse: 1.7977\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 83ms/step - loss: 2.1563 - mse: 2.1563 - val_loss: 1.8100 - val_mse: 1.8100\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 2.1597 - mse: 2.1597 - val_loss: 1.7832 - val_mse: 1.7832\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 185ms/step - loss: 2.9578 - mse: 2.9578 - val_loss: 1.8415 - val_mse: 1.8415\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.4056 - mse: 2.4056 - val_loss: 1.8768 - val_mse: 1.8768\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.2262 - mse: 2.2262 - val_loss: 1.8336 - val_mse: 1.8336\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 2.2940 - mse: 2.2940 - val_loss: 1.9876 - val_mse: 1.9876\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.1773 - mse: 2.1773 - val_loss: 1.8268 - val_mse: 1.8268\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.2356 - mse: 2.2356 - val_loss: 1.8198 - val_mse: 1.8198\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 2.2117 - mse: 2.2117 - val_loss: 1.8067 - val_mse: 1.8067\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 2.1580 - mse: 2.1580 - val_loss: 1.8152 - val_mse: 1.8152\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 72ms/step - loss: 2.1557 - mse: 2.1557 - val_loss: 1.7875 - val_mse: 1.7875\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 73ms/step - loss: 2.1383 - mse: 2.1383 - val_loss: 1.7385 - val_mse: 1.7385\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 189ms/step - loss: 2.7665 - mse: 2.7665 - val_loss: 1.8972 - val_mse: 1.8972\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.2951 - mse: 2.2951 - val_loss: 1.8414 - val_mse: 1.8414\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.2078 - mse: 2.2078 - val_loss: 1.8231 - val_mse: 1.8231\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1743 - mse: 2.1743 - val_loss: 1.9042 - val_mse: 1.9042\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 2.2087 - mse: 2.2087 - val_loss: 1.8041 - val_mse: 1.8041\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1696 - mse: 2.1696 - val_loss: 1.7649 - val_mse: 1.7649\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1840 - mse: 2.1840 - val_loss: 1.7502 - val_mse: 1.7502\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 2.1127 - mse: 2.1127 - val_loss: 1.7533 - val_mse: 1.7533\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1706 - mse: 2.1706 - val_loss: 1.6757 - val_mse: 1.6757\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.0362 - mse: 2.0362 - val_loss: 1.6174 - val_mse: 1.6174\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 192ms/step - loss: 3.0651 - mse: 3.0651 - val_loss: 1.9971 - val_mse: 1.9971\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.3511 - mse: 2.3511 - val_loss: 1.8992 - val_mse: 1.8992\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 2.1784 - mse: 2.1784 - val_loss: 1.8195 - val_mse: 1.8195\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.2105 - mse: 2.2105 - val_loss: 1.8147 - val_mse: 1.8147\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1929 - mse: 2.1929 - val_loss: 1.8122 - val_mse: 1.8122\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1815 - mse: 2.1815 - val_loss: 1.8629 - val_mse: 1.8629\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.2050 - mse: 2.2050 - val_loss: 1.8572 - val_mse: 1.8572\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1436 - mse: 2.1436 - val_loss: 1.8011 - val_mse: 1.8011\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1859 - mse: 2.1859 - val_loss: 1.7900 - val_mse: 1.7900\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1804 - mse: 2.1804 - val_loss: 1.7816 - val_mse: 1.7816\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 158ms/step - loss: 2.9710 - mse: 2.9710 - val_loss: 1.8135 - val_mse: 1.8135\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2871 - mse: 2.2871 - val_loss: 1.8021 - val_mse: 1.8021\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.2125 - mse: 2.2125 - val_loss: 1.7913 - val_mse: 1.7913\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1806 - mse: 2.1806 - val_loss: 1.8419 - val_mse: 1.8419\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1307 - mse: 2.1307 - val_loss: 1.7564 - val_mse: 1.7564\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1664 - mse: 2.1664 - val_loss: 1.7263 - val_mse: 1.7263\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1144 - mse: 2.1144 - val_loss: 1.8324 - val_mse: 1.8324\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.0912 - mse: 2.0912 - val_loss: 1.6279 - val_mse: 1.6279\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.0575 - mse: 2.0575 - val_loss: 1.5744 - val_mse: 1.5744\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.8934 - mse: 1.8934 - val_loss: 1.5416 - val_mse: 1.5416\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 164ms/step - loss: 2.9394 - mse: 2.9394 - val_loss: 1.8488 - val_mse: 1.8488\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.4024 - mse: 2.4024 - val_loss: 1.8326 - val_mse: 1.8326\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.2213 - mse: 2.2213 - val_loss: 1.8205 - val_mse: 1.8205\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.2134 - mse: 2.2134 - val_loss: 1.8654 - val_mse: 1.8654\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1809 - mse: 2.1809 - val_loss: 1.8141 - val_mse: 1.8141\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1747 - mse: 2.1747 - val_loss: 1.8194 - val_mse: 1.8194\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1594 - mse: 2.1594 - val_loss: 1.8055 - val_mse: 1.8055\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1516 - mse: 2.1516 - val_loss: 1.7794 - val_mse: 1.7794\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1736 - mse: 2.1736 - val_loss: 1.7287 - val_mse: 1.7287\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1331 - mse: 2.1331 - val_loss: 1.6849 - val_mse: 1.6849\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 157ms/step - loss: 2.9907 - mse: 2.9907 - val_loss: 1.8770 - val_mse: 1.8770\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.2441 - mse: 2.2441 - val_loss: 1.8565 - val_mse: 1.8565\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1844 - mse: 2.1844 - val_loss: 1.8115 - val_mse: 1.8115\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1954 - mse: 2.1954 - val_loss: 1.7929 - val_mse: 1.7929\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1975 - mse: 2.1975 - val_loss: 1.8283 - val_mse: 1.8283\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1402 - mse: 2.1402 - val_loss: 1.7615 - val_mse: 1.7615\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1250 - mse: 2.1250 - val_loss: 1.7369 - val_mse: 1.7369\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1126 - mse: 2.1126 - val_loss: 1.7049 - val_mse: 1.7049\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.0523 - mse: 2.0523 - val_loss: 1.5850 - val_mse: 1.5850\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.9030 - mse: 1.9030 - val_loss: 1.5118 - val_mse: 1.5118\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 157ms/step - loss: 3.0456 - mse: 3.0456 - val_loss: 1.9045 - val_mse: 1.9045\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2419 - mse: 2.2419 - val_loss: 1.8338 - val_mse: 1.8338\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 2.2410 - mse: 2.2410 - val_loss: 1.8314 - val_mse: 1.8314\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1714 - mse: 2.1714 - val_loss: 1.8156 - val_mse: 1.8156\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1681 - mse: 2.1681 - val_loss: 1.8029 - val_mse: 1.8029\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 59ms/step - loss: 2.1539 - mse: 2.1539 - val_loss: 1.8015 - val_mse: 1.8015\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1381 - mse: 2.1381 - val_loss: 1.7355 - val_mse: 1.7355\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1300 - mse: 2.1300 - val_loss: 1.6583 - val_mse: 1.6583\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1111 - mse: 2.1111 - val_loss: 1.5379 - val_mse: 1.5379\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.0401 - mse: 2.0401 - val_loss: 2.0683 - val_mse: 2.0683\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 168ms/step - loss: 2.8999 - mse: 2.8999 - val_loss: 1.8393 - val_mse: 1.8393\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.2202 - mse: 2.2202 - val_loss: 1.8057 - val_mse: 1.8057\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.2044 - mse: 2.2044 - val_loss: 1.7965 - val_mse: 1.7965\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1595 - mse: 2.1595 - val_loss: 1.8549 - val_mse: 1.8549\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1706 - mse: 2.1706 - val_loss: 1.7277 - val_mse: 1.7277\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.0856 - mse: 2.0856 - val_loss: 1.6823 - val_mse: 1.6823\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 1.9688 - mse: 1.9688 - val_loss: 1.8035 - val_mse: 1.8035\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.8905 - mse: 1.8905 - val_loss: 1.4222 - val_mse: 1.4222\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.7554 - mse: 1.7554 - val_loss: 1.3391 - val_mse: 1.3391\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 1.9296 - mse: 1.9296 - val_loss: 1.4111 - val_mse: 1.4111\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 163ms/step - loss: 2.8276 - mse: 2.8276 - val_loss: 1.8281 - val_mse: 1.8281\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.3125 - mse: 2.3125 - val_loss: 1.8217 - val_mse: 1.8217\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1860 - mse: 2.1860 - val_loss: 1.8120 - val_mse: 1.8120\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1942 - mse: 2.1942 - val_loss: 1.8268 - val_mse: 1.8268\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1594 - mse: 2.1594 - val_loss: 1.7805 - val_mse: 1.7805\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1794 - mse: 2.1794 - val_loss: 1.7691 - val_mse: 1.7691\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1937 - mse: 2.1937 - val_loss: 1.8482 - val_mse: 1.8482\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1360 - mse: 2.1360 - val_loss: 1.7228 - val_mse: 1.7228\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1081 - mse: 2.1081 - val_loss: 1.7297 - val_mse: 1.7297\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1171 - mse: 2.1171 - val_loss: 1.6904 - val_mse: 1.6904\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 162ms/step - loss: 2.9540 - mse: 2.9540 - val_loss: 2.0386 - val_mse: 2.0386\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.2825 - mse: 2.2825 - val_loss: 1.8354 - val_mse: 1.8354\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.2238 - mse: 2.2238 - val_loss: 1.8293 - val_mse: 1.8293\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1865 - mse: 2.1865 - val_loss: 1.9100 - val_mse: 1.9100\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1932 - mse: 2.1932 - val_loss: 1.8253 - val_mse: 1.8253\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1736 - mse: 2.1736 - val_loss: 1.8065 - val_mse: 1.8065\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1609 - mse: 2.1609 - val_loss: 1.8044 - val_mse: 1.8044\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1590 - mse: 2.1590 - val_loss: 1.7706 - val_mse: 1.7706\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1104 - mse: 2.1104 - val_loss: 1.6914 - val_mse: 1.6914\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.0660 - mse: 2.0660 - val_loss: 1.5973 - val_mse: 1.5973\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 161ms/step - loss: 2.9225 - mse: 2.9225 - val_loss: 1.8329 - val_mse: 1.8329\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.4095 - mse: 2.4095 - val_loss: 1.8258 - val_mse: 1.8258\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 2.2060 - mse: 2.2060 - val_loss: 1.8211 - val_mse: 1.8211\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1741 - mse: 2.1741 - val_loss: 1.8462 - val_mse: 1.8462\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.2511 - mse: 2.2511 - val_loss: 1.9128 - val_mse: 1.9128\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1775 - mse: 2.1775 - val_loss: 1.8345 - val_mse: 1.8345\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1894 - mse: 2.1894 - val_loss: 1.8084 - val_mse: 1.8084\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1777 - mse: 2.1777 - val_loss: 1.8137 - val_mse: 1.8137\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1657 - mse: 2.1657 - val_loss: 1.8338 - val_mse: 1.8338\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1634 - mse: 2.1634 - val_loss: 1.8114 - val_mse: 1.8114\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 165ms/step - loss: 2.9420 - mse: 2.9420 - val_loss: 1.8271 - val_mse: 1.8271\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 64ms/step - loss: 2.3218 - mse: 2.3218 - val_loss: 1.8244 - val_mse: 1.8244\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1926 - mse: 2.1926 - val_loss: 1.8240 - val_mse: 1.8240\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1722 - mse: 2.1722 - val_loss: 1.8160 - val_mse: 1.8160\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1721 - mse: 2.1721 - val_loss: 1.7925 - val_mse: 1.7925\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1511 - mse: 2.1511 - val_loss: 1.7874 - val_mse: 1.7874\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1286 - mse: 2.1286 - val_loss: 1.7483 - val_mse: 1.7483\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1264 - mse: 2.1264 - val_loss: 1.6729 - val_mse: 1.6729\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.0722 - mse: 2.0722 - val_loss: 1.7310 - val_mse: 1.7310\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.0889 - mse: 2.0889 - val_loss: 1.5815 - val_mse: 1.5815\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 164ms/step - loss: 2.9584 - mse: 2.9584 - val_loss: 1.8561 - val_mse: 1.8561\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.3064 - mse: 2.3064 - val_loss: 1.8473 - val_mse: 1.8473\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1888 - mse: 2.1888 - val_loss: 1.8272 - val_mse: 1.8272\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1879 - mse: 2.1879 - val_loss: 1.8104 - val_mse: 1.8104\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1639 - mse: 2.1639 - val_loss: 1.8061 - val_mse: 1.8061\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1712 - mse: 2.1712 - val_loss: 1.7892 - val_mse: 1.7892\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1172 - mse: 2.1172 - val_loss: 1.7316 - val_mse: 1.7316\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1478 - mse: 2.1478 - val_loss: 1.7010 - val_mse: 1.7010\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 2.2565 - mse: 2.2565 - val_loss: 1.6878 - val_mse: 1.6878\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1577 - mse: 2.1577 - val_loss: 1.7117 - val_mse: 1.7117\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 170ms/step - loss: 2.8204 - mse: 2.8204 - val_loss: 1.8887 - val_mse: 1.8887\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.4525 - mse: 2.4525 - val_loss: 1.8302 - val_mse: 1.8302\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.2308 - mse: 2.2308 - val_loss: 1.8191 - val_mse: 1.8191\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1805 - mse: 2.1805 - val_loss: 1.8444 - val_mse: 1.8444\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1653 - mse: 2.1653 - val_loss: 1.8777 - val_mse: 1.8777\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1987 - mse: 2.1987 - val_loss: 1.8534 - val_mse: 1.8534\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1503 - mse: 2.1503 - val_loss: 1.7748 - val_mse: 1.7748\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1595 - mse: 2.1595 - val_loss: 1.7562 - val_mse: 1.7562\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1178 - mse: 2.1178 - val_loss: 1.7680 - val_mse: 1.7680\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1152 - mse: 2.1152 - val_loss: 1.6920 - val_mse: 1.6920\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 164ms/step - loss: 2.8782 - mse: 2.8782 - val_loss: 1.8375 - val_mse: 1.8375\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2438 - mse: 2.2438 - val_loss: 1.8516 - val_mse: 1.8516\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.4599 - mse: 2.4599 - val_loss: 1.9092 - val_mse: 1.9092\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1769 - mse: 2.1769 - val_loss: 1.8272 - val_mse: 1.8272\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2527 - mse: 2.2527 - val_loss: 1.7918 - val_mse: 1.7918\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1756 - mse: 2.1756 - val_loss: 1.8028 - val_mse: 1.8028\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 1s 78ms/step - loss: 2.1938 - mse: 2.1938 - val_loss: 1.8263 - val_mse: 1.8263\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1250 - mse: 2.1250 - val_loss: 1.7432 - val_mse: 1.7432\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1195 - mse: 2.1195 - val_loss: 1.6994 - val_mse: 1.6994\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.0914 - mse: 2.0914 - val_loss: 1.6485 - val_mse: 1.6485\n",
      "********************************* 当前迭代次数:  2 ***************************************\n",
      "最好的适应度数值:  0.77784475\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 178ms/step - loss: 2.8910 - mse: 2.8910 - val_loss: 1.8730 - val_mse: 1.8730\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.8290 - mse: 2.8290 - val_loss: 1.8526 - val_mse: 1.8526\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 73ms/step - loss: 2.2002 - mse: 2.2002 - val_loss: 1.8829 - val_mse: 1.8829\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 2.3039 - mse: 2.3039 - val_loss: 1.8300 - val_mse: 1.8300\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 2.2182 - mse: 2.2182 - val_loss: 1.8448 - val_mse: 1.8448\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1769 - mse: 2.1769 - val_loss: 1.8406 - val_mse: 1.8406\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1709 - mse: 2.1709 - val_loss: 1.8349 - val_mse: 1.8349\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1940 - mse: 2.1940 - val_loss: 1.8439 - val_mse: 1.8439\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.1505 - mse: 2.1505 - val_loss: 1.7933 - val_mse: 1.7933\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1825 - mse: 2.1825 - val_loss: 1.7725 - val_mse: 1.7725\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 168ms/step - loss: 2.9413 - mse: 2.9413 - val_loss: 1.8861 - val_mse: 1.8861\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.5387 - mse: 2.5387 - val_loss: 2.1370 - val_mse: 2.1370\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.2302 - mse: 2.2302 - val_loss: 1.8170 - val_mse: 1.8170\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.1923 - mse: 2.1923 - val_loss: 1.8171 - val_mse: 1.8171\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1882 - mse: 2.1882 - val_loss: 1.8165 - val_mse: 1.8165\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 72ms/step - loss: 2.1803 - mse: 2.1803 - val_loss: 1.8244 - val_mse: 1.8244\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 2.1812 - mse: 2.1812 - val_loss: 1.8452 - val_mse: 1.8452\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 70ms/step - loss: 2.1813 - mse: 2.1813 - val_loss: 1.8433 - val_mse: 1.8433\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.1759 - mse: 2.1759 - val_loss: 1.8486 - val_mse: 1.8486\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1802 - mse: 2.1802 - val_loss: 1.8412 - val_mse: 1.8412\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 4s 167ms/step - loss: 2.8263 - mse: 2.8263 - val_loss: 1.8491 - val_mse: 1.8491\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.3534 - mse: 2.3534 - val_loss: 1.8078 - val_mse: 1.8078\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1978 - mse: 2.1978 - val_loss: 1.8298 - val_mse: 1.8298\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1757 - mse: 2.1757 - val_loss: 1.7774 - val_mse: 1.7774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1478 - mse: 2.1478 - val_loss: 1.7675 - val_mse: 1.7675\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.0944 - mse: 2.0944 - val_loss: 1.7044 - val_mse: 1.7044\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1102 - mse: 2.1102 - val_loss: 1.6488 - val_mse: 1.6488\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.0358 - mse: 2.0358 - val_loss: 1.5647 - val_mse: 1.5647\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 1.9239 - mse: 1.9239 - val_loss: 1.4385 - val_mse: 1.4385\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 1.8114 - mse: 1.8114 - val_loss: 1.3809 - val_mse: 1.3809\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 156ms/step - loss: 2.9986 - mse: 2.9986 - val_loss: 1.8304 - val_mse: 1.8304\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.3428 - mse: 2.3428 - val_loss: 1.8246 - val_mse: 1.8246\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.2353 - mse: 2.2353 - val_loss: 1.8328 - val_mse: 1.8328\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1609 - mse: 2.1609 - val_loss: 1.8038 - val_mse: 1.8038\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1961 - mse: 2.1961 - val_loss: 1.7917 - val_mse: 1.7917\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1990 - mse: 2.1990 - val_loss: 1.8512 - val_mse: 1.8512\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1902 - mse: 2.1902 - val_loss: 1.7673 - val_mse: 1.7673\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1339 - mse: 2.1339 - val_loss: 1.7521 - val_mse: 1.7521\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.0972 - mse: 2.0972 - val_loss: 1.7145 - val_mse: 1.7145\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.0617 - mse: 2.0617 - val_loss: 1.5783 - val_mse: 1.5783\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 160ms/step - loss: 2.8824 - mse: 2.8824 - val_loss: 1.9111 - val_mse: 1.9111\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 2.2709 - mse: 2.2709 - val_loss: 1.8413 - val_mse: 1.8413\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2025 - mse: 2.2025 - val_loss: 1.8346 - val_mse: 1.8346\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1949 - mse: 2.1949 - val_loss: 1.8461 - val_mse: 1.8461\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2231 - mse: 2.2231 - val_loss: 1.9036 - val_mse: 1.9036\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.1521 - mse: 2.1521 - val_loss: 1.7933 - val_mse: 1.7933\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1984 - mse: 2.1984 - val_loss: 1.7819 - val_mse: 1.7819\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.2025 - mse: 2.2025 - val_loss: 1.8255 - val_mse: 1.8255\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1613 - mse: 2.1613 - val_loss: 1.7465 - val_mse: 1.7465\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1178 - mse: 2.1178 - val_loss: 1.6877 - val_mse: 1.6877\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 160ms/step - loss: 2.9070 - mse: 2.9070 - val_loss: 1.8475 - val_mse: 1.8475\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 60ms/step - loss: 2.3451 - mse: 2.3451 - val_loss: 1.8243 - val_mse: 1.8243\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1931 - mse: 2.1931 - val_loss: 1.8456 - val_mse: 1.8456\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1754 - mse: 2.1754 - val_loss: 1.8207 - val_mse: 1.8207\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1595 - mse: 2.1595 - val_loss: 1.8194 - val_mse: 1.8194\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1623 - mse: 2.1623 - val_loss: 1.7684 - val_mse: 1.7684\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.1368 - mse: 2.1368 - val_loss: 1.7379 - val_mse: 1.7379\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.0399 - mse: 2.0399 - val_loss: 1.6146 - val_mse: 1.6146\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 1.9168 - mse: 1.9168 - val_loss: 1.6081 - val_mse: 1.6081\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 1.9178 - mse: 1.9178 - val_loss: 1.7690 - val_mse: 1.7690\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 163ms/step - loss: 2.9690 - mse: 2.9690 - val_loss: 1.8312 - val_mse: 1.8312\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.3573 - mse: 2.3573 - val_loss: 1.8219 - val_mse: 1.8219\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2820 - mse: 2.2820 - val_loss: 1.8233 - val_mse: 1.8233\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1660 - mse: 2.1660 - val_loss: 1.8722 - val_mse: 1.8722\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.2515 - mse: 2.2515 - val_loss: 1.9455 - val_mse: 1.9455\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1982 - mse: 2.1982 - val_loss: 1.8187 - val_mse: 1.8187\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 61ms/step - loss: 2.1679 - mse: 2.1679 - val_loss: 1.8027 - val_mse: 1.8027\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1737 - mse: 2.1737 - val_loss: 1.8012 - val_mse: 1.8012\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1517 - mse: 2.1517 - val_loss: 1.7967 - val_mse: 1.7967\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 62ms/step - loss: 2.1482 - mse: 2.1482 - val_loss: 1.7883 - val_mse: 1.7883\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 160ms/step - loss: 2.9160 - mse: 2.9160 - val_loss: 1.8606 - val_mse: 1.8606\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.4556 - mse: 2.4556 - val_loss: 1.8353 - val_mse: 1.8353\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2266 - mse: 2.2266 - val_loss: 1.8241 - val_mse: 1.8241\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.2150 - mse: 2.2150 - val_loss: 1.8692 - val_mse: 1.8692\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1885 - mse: 2.1885 - val_loss: 1.8536 - val_mse: 1.8536\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1819 - mse: 2.1819 - val_loss: 1.8098 - val_mse: 1.8098\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1949 - mse: 2.1949 - val_loss: 1.8021 - val_mse: 1.8021\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.1623 - mse: 2.1623 - val_loss: 1.8096 - val_mse: 1.8096\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1619 - mse: 2.1619 - val_loss: 1.8407 - val_mse: 1.8407\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 2.1421 - mse: 2.1421 - val_loss: 1.7540 - val_mse: 1.7540\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 163ms/step - loss: 2.9454 - mse: 2.9454 - val_loss: 1.9350 - val_mse: 1.9350\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2506 - mse: 2.2506 - val_loss: 1.8565 - val_mse: 1.8565\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.2667 - mse: 2.2667 - val_loss: 1.8984 - val_mse: 1.8984\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.2078 - mse: 2.2078 - val_loss: 1.8209 - val_mse: 1.8209\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.2112 - mse: 2.2112 - val_loss: 1.8136 - val_mse: 1.8136\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 65ms/step - loss: 2.1677 - mse: 2.1677 - val_loss: 1.8363 - val_mse: 1.8363\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 63ms/step - loss: 2.1696 - mse: 2.1696 - val_loss: 1.8346 - val_mse: 1.8346\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 68ms/step - loss: 2.1868 - mse: 2.1868 - val_loss: 1.7918 - val_mse: 1.7918\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 66ms/step - loss: 2.1796 - mse: 2.1796 - val_loss: 1.8734 - val_mse: 1.8734\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.1605 - mse: 2.1605 - val_loss: 1.7826 - val_mse: 1.7826\n",
      "Epoch 1/10\n",
      "7/7 [==============================] - 3s 176ms/step - loss: 2.9808 - mse: 2.9808 - val_loss: 1.8672 - val_mse: 1.8672\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 69ms/step - loss: 2.2943 - mse: 2.2943 - val_loss: 1.8377 - val_mse: 1.8377\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 74ms/step - loss: 2.2902 - mse: 2.2902 - val_loss: 1.8546 - val_mse: 1.8546\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 73ms/step - loss: 2.2484 - mse: 2.2484 - val_loss: 1.8418 - val_mse: 1.8418\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 74ms/step - loss: 2.1729 - mse: 2.1729 - val_loss: 1.8479 - val_mse: 1.8479\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 72ms/step - loss: 2.1708 - mse: 2.1708 - val_loss: 1.8361 - val_mse: 1.8361\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 67ms/step - loss: 2.2041 - mse: 2.2041 - val_loss: 1.8079 - val_mse: 1.8079\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 71ms/step - loss: 2.1355 - mse: 2.1355 - val_loss: 1.7644 - val_mse: 1.7644\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 1s 76ms/step - loss: 2.1494 - mse: 2.1494 - val_loss: 1.7415 - val_mse: 1.7415\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 64ms/step - loss: 2.0997 - mse: 2.0997 - val_loss: 1.6740 - val_mse: 1.6740\n",
      "----------------HHO哈里斯鹰优化算法优化LSTM模型-最优结果展示-----------------\n",
      "The best units is 48\n",
      "The best epochs is 60\n"
     ]
    }
   ],
   "source": [
    "from numpy.random import rand\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import keras.layers as layers  \n",
    "import keras.backend as K \n",
    "from keras.optimizers import adam_v2\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM,Dropout\n",
    "from numpy import loadtxt\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score, mean_absolute_error, r2_score \n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, BatchNormalization\n",
    "import math\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 定义初始化位置函数\n",
    "def init_position(lb, ub, N, dim):\n",
    "    X = np.zeros([N, dim], dtype='float')  # 位置初始化为0\n",
    "    for i in range(N):  # 循环\n",
    "        for d in range(dim):  # 循环\n",
    "            X[i, d] = lb[0, d] + (ub[0, d] - lb[0, d]) * rand()  # 位置随机初始化\n",
    "\n",
    "    return X  # 返回位置数据\n",
    "\n",
    "\n",
    "# 定义转换函数\n",
    "def binary_conversion(X, thres, N, dim):\n",
    "    Xbin = np.zeros([N, dim], dtype='int')  # 位置初始化为0\n",
    "    for i in range(N):  # 循环\n",
    "        for d in range(dim):  # 循环\n",
    "            if X[i, d] > thres:  # 判断\n",
    "                Xbin[i, d] = 1  # 赋值\n",
    "            else:\n",
    "                Xbin[i, d] = 0  # 赋值\n",
    "\n",
    "    return Xbin  # 返回数据\n",
    "\n",
    "\n",
    "# 定义边界处理函数\n",
    "def boundary(x, lb, ub):\n",
    "    if x < lb:  # 小于最小值\n",
    "        x = lb  # 赋值最小值\n",
    "    if x > ub:  # 大于最大值\n",
    "        x = ub  # 赋值最大值\n",
    "\n",
    "    return x  # 返回位置数据\n",
    "\n",
    "\n",
    "# 定义莱维飞行函数\n",
    "def levy_distribution(beta, dim):\n",
    "    # Sigma计算赋值\n",
    "    nume = math.gamma(1 + beta) * np.sin(np.pi * beta / 2)  # 计算\n",
    "    deno = math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2)  # 计算\n",
    "    sigma = (nume / deno) ** (1 / beta)  # Sigma赋值\n",
    "    # Parameter u & v\n",
    "    u = np.random.randn(dim) * sigma  # u参数随机赋值\n",
    "    v = np.random.randn(dim)  # v参数随机赋值\n",
    "    # 计算步骤\n",
    "    step = u / abs(v) ** (1 / beta)  # 计算\n",
    "    LF = 0.01 * step  # LF赋值\n",
    "\n",
    "    return LF  # 返回数据\n",
    "\n",
    "\n",
    "# 定义错误率计算函数\n",
    "def error_rate(X_train, y_train, X_test, y_test, x, opts):\n",
    "    if abs(x[0]) > 0:  # 判断取值\n",
    "        units = int(abs(x[0])) * 10  # 赋值\n",
    "    else:\n",
    "        units = int(abs(x[0])) + 16  # 赋值\n",
    "\n",
    "    if abs(x[1]) > 0:  # 判断取值\n",
    "        epochs = int(abs(x[1])) * 10  # 赋值\n",
    "    else:\n",
    "        epochs = int(abs(x[1])) + 10  # 赋值\n",
    "\n",
    "    # 建支持LSTM模型并训练\n",
    "    lstm = Sequential()  # 序贯模型\n",
    "    lstm.add(LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], 1)))  # LSTM层\n",
    "    lstm.add(LSTM(units=64, return_sequences=True))\n",
    "    lstm.add(LSTM(units=32))\n",
    "    lstm.add(Dense(units, activation='tanh'))  # 全连接层\n",
    "    lstm.add(Dense(units/2, activation='tanh'))  # 全连接层\n",
    "    lstm.add(Dense(1))  # 输出层\n",
    "    lstm.compile(loss='mean_squared_error',\n",
    "                 optimizer=adam_v2.Adam(learning_rate=0.001),\n",
    "                 metrics=['mse'])  # 编译\n",
    "    lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=16)  # 拟合\n",
    "    y_pred = lstm.predict(X_test, batch_size=10)  # 预测\n",
    "    score = round(r2_score(y_test, y_pred), 4)  # 计算R2\n",
    "\n",
    "    # 使错误率降到最低\n",
    "    fitness_value = 1 - score  # 错误率 赋值 适应度函数值\n",
    "\n",
    "    return fitness_value  # 返回适应度\n",
    "\n",
    "\n",
    "# 定义目标函数\n",
    "def Fun(X_train, y_train, X_test, y_test, x, opts):\n",
    "    # 参数\n",
    "    alpha = 0.99  # 赋值\n",
    "    beta = 1 - alpha  # 赋值\n",
    "    # 原始特征数\n",
    "    max_feat = len(x)\n",
    "    # 选择特征数\n",
    "    num_feat = np.sum(x == 1)\n",
    "    # 无特征选择判断\n",
    "    if num_feat == 0:  # 判断\n",
    "        cost = 1  # 赋值\n",
    "    else:\n",
    "        # 调用错误率计算函数\n",
    "        error = error_rate(X_train, y_train, X_test, y_test, x, opts)\n",
    "        # 目标函数计算\n",
    "        cost = alpha * error + beta * (num_feat / max_feat)\n",
    "\n",
    "    return cost  # 返回数据\n",
    "\n",
    "\n",
    "# 定义哈里斯鹰优化算法主函数\n",
    "def jfs(X_train, y_train, X_test, y_test, opts):\n",
    "    # 参数\n",
    "    ub = 1  # 上限\n",
    "    lb = 0  # 下限\n",
    "    thres = 0.5  # 阀值\n",
    "    beta = 1.5  # levy 参数\n",
    "\n",
    "    N = opts['N']  # 种群数量\n",
    "    max_iter = opts['T']  # 最大迭代次数\n",
    "    if 'beta' in opts:  # 判断\n",
    "        beta = opts['beta']  # 赋值\n",
    "\n",
    "    # 维度\n",
    "    dim = np.size(X_train, 1)  # 获取维度\n",
    "    if np.size(lb) == 1:  # 判断\n",
    "        ub = ub * np.ones([1, dim], dtype='float')  # 初始化上限为1\n",
    "        lb = lb * np.ones([1, dim], dtype='float')  # 初始化下限为1\n",
    "\n",
    "    # 调用位置初始化函数\n",
    "    X = init_position(lb, ub, N, dim)\n",
    "\n",
    "    fit = np.zeros([N, 1], dtype='float')  # 适应度初始化为0\n",
    "    Xrb = np.zeros([1, dim], dtype='float')  # 猎物位置初始化为0\n",
    "    fitR = float('inf')  # 初始化为无穷\n",
    "\n",
    "    curve = np.zeros([1, max_iter], dtype='float')  # 适应度初始化为0\n",
    "    t = 0  # 赋值\n",
    "\n",
    "    while t < max_iter:  # 循环\n",
    "        # 调用转换函数\n",
    "        Xbin = binary_conversion(X, thres, N, dim)\n",
    "\n",
    "        # 计算适应度\n",
    "        for i in range(N):  # 循环\n",
    "            fit[i, 0] = Fun(X_train, y_train, X_test, y_test, Xbin[i, :], opts)  # 调用目标函数\n",
    "            if fit[i, 0] < fitR:  # 判断\n",
    "                Xrb[0, :] = X[i, :]  # 猎物位置赋值\n",
    "                fitR = fit[i, 0]  # 适应度赋值\n",
    "\n",
    "        # 存储结果\n",
    "        curve[0, t] = fitR.copy()  # 复制\n",
    "        print(\"*********************************\", \"当前迭代次数: \", t + 1, \"***************************************\")\n",
    "        print(\"最好的适应度数值: \", curve[0, t])\n",
    "        t += 1\n",
    "\n",
    "        # 平均位置\n",
    "        X_mu = np.zeros([1, dim], dtype='float')  # 初始化为0\n",
    "        X_mu[0, :] = np.mean(X, axis=0)  # 计算平均位置\n",
    "\n",
    "        for i in range(N):  # 循环\n",
    "            E0 = -1 + 2 * rand()  # 猎物的初始能量  [-1,1] 之间的随机数\n",
    "            E = 2 * E0 * (1 - (t / max_iter))  # 逃逸能量\n",
    "            # 当|E|≥1 时进入搜索阶段\n",
    "            if abs(E) >= 1:\n",
    "                q = rand()  # 生成随机数 [0,1]\n",
    "                if q >= 0.5:  # 判断\n",
    "                    k = np.random.randint(low=0, high=N)  # 生成随机整数  个体\n",
    "                    r1 = rand()  # [0,1]之间的随机数\n",
    "                    r2 = rand()  # [0,1]之间的随机数\n",
    "                    for d in range(dim):  # 循环\n",
    "                        X[i, d] = X[k, d] - r1 * abs(X[k, d] - 2 * r2 * X[i, d])  # 更新位置\n",
    "                        X[i, d] = boundary(X[i, d], lb[0, d], ub[0, d])  # 边界处理\n",
    "\n",
    "                elif q < 0.5:  # 判断\n",
    "                    r3 = rand()  # [0,1]之间的随机数\n",
    "                    r4 = rand()  # [0,1]之间的随机数\n",
    "                    for d in range(dim):  # 循环\n",
    "                        X[i, d] = (Xrb[0, d] - X_mu[0, d]) - r3 * (lb[0, d] + r4 * (ub[0, d] - lb[0, d]))  # 更新位置\n",
    "                        X[i, d] = boundary(X[i, d], lb[0, d], ub[0, d])  # 边界处理\n",
    "\n",
    "\n",
    "            elif abs(E) < 1:  # 开发阶段\n",
    "                J = 2 * (1 - rand())  # 生成随机数\n",
    "                r = rand()  # 生成随机数\n",
    "                # 软围攻策略进行位置更新\n",
    "                if r >= 0.5 and abs(E) >= 0.5:\n",
    "                    for d in range(dim):  # 循环\n",
    "                        DX = Xrb[0, d] - X[i, d]  # 猎物位置与个体当前位置的差值\n",
    "                        X[i, d] = DX - E * abs(J * Xrb[0, d] - X[i, d])  # 更新位置\n",
    "                        X[i, d] = boundary(X[i, d], lb[0, d], ub[0, d])  # 边界处理\n",
    "\n",
    "                # 硬围攻策略进行位置更新\n",
    "                elif r >= 0.5 and abs(E) < 0.5:\n",
    "                    for d in range(dim):  # 循环\n",
    "                        DX = Xrb[0, d] - X[i, d]  # 猎物位置与个体当前位置的差值\n",
    "                        X[i, d] = Xrb[0, d] - E * abs(DX)  # 更新位置\n",
    "                        X[i, d] = boundary(X[i, d], lb[0, d], ub[0, d])  # 边界处理\n",
    "\n",
    "                # 渐近式快速俯冲的软包围策略进行位置更新\n",
    "                elif r < 0.5 and abs(E) >= 0.5:\n",
    "                    LF = levy_distribution(beta, dim)  # 莱维飞行\n",
    "                    Y = np.zeros([1, dim], dtype='float')  # 初始化为1\n",
    "                    Z = np.zeros([1, dim], dtype='float')  # 初始化为1\n",
    "\n",
    "                    for d in range(dim):  # 循环\n",
    "\n",
    "                        Y[0, d] = Xrb[0, d] - E * abs(J * Xrb[0, d] - X[i, d])  # 更新位置\n",
    "\n",
    "                        Y[0, d] = boundary(Y[0, d], lb[0, d], ub[0, d])  # 边界处理\n",
    "\n",
    "                    for d in range(dim):  # 循环\n",
    "\n",
    "                        Z[0, d] = Y[0, d] + rand() * LF[d]  # 更新位置\n",
    "\n",
    "                        Z[0, d] = boundary(Z[0, d], lb[0, d], ub[0, d])  # 边界处理\n",
    "\n",
    "                        # 调用转换函数\n",
    "                    Ybin = binary_conversion(Y, thres, 1, dim)\n",
    "                    Zbin = binary_conversion(Z, thres, 1, dim)\n",
    "                    # 适应度计算\n",
    "                    fitY = Fun(X_train, y_train, X_test, y_test, Ybin[0, :], opts)\n",
    "                    fitZ = Fun(X_train, y_train, X_test, y_test, Zbin[0, :], opts)\n",
    "                    # 根据适应度进行判断\n",
    "                    if fitY < fit[i, 0]:\n",
    "                        fit[i, 0] = fitY  # 赋值\n",
    "                        X[i, :] = Y[0, :]  # 赋值\n",
    "                    if fitZ < fit[i, 0]:\n",
    "                        fit[i, 0] = fitZ  # 赋值\n",
    "                        X[i, :] = Z[0, :]  # 赋值\n",
    "\n",
    "                # 带有莱维飞行的硬围攻策略进行位置更新\n",
    "                elif r < 0.5 and abs(E) < 0.5:\n",
    "                    # Levy distribution (9)\n",
    "                    LF = levy_distribution(beta, dim)  # 莱维飞行\n",
    "                    Y = np.zeros([1, dim], dtype='float')  # 初始化为0\n",
    "                    Z = np.zeros([1, dim], dtype='float')  # 初始化为0\n",
    "\n",
    "                    for d in range(dim):  # 循环\n",
    "\n",
    "                        Y[0, d] = Xrb[0, d] - E * abs(J * Xrb[0, d] - X_mu[0, d])  # 更新位置\n",
    "\n",
    "                        Y[0, d] = boundary(Y[0, d], lb[0, d], ub[0, d])  # 边界处理\n",
    "\n",
    "                    for d in range(dim):  # 循环\n",
    "\n",
    "                        Z[0, d] = Y[0, d] + rand() * LF[d]  # 更新位置\n",
    "\n",
    "                        Z[0, d] = boundary(Z[0, d], lb[0, d], ub[0, d])  # 边界处理\n",
    "\n",
    "                        # 调用转换函数\n",
    "                    Ybin = binary_conversion(Y, thres, 1, dim)\n",
    "                    Zbin = binary_conversion(Z, thres, 1, dim)\n",
    "                    # 适应度计算\n",
    "                    fitY = Fun(X_train, y_train, X_test, y_test, Ybin[0, :], opts)\n",
    "                    fitZ = Fun(X_train, y_train, X_test, y_test, Zbin[0, :], opts)\n",
    "                    # 根据适应度进行判断\n",
    "                    if fitY < fit[i, 0]:\n",
    "                        fit[i, 0] = fitY  # 赋值\n",
    "                        X[i, :] = Y[0, :]  # 赋值\n",
    "                    if fitZ < fit[i, 0]:\n",
    "                        fit[i, 0] = fitZ  # 赋值\n",
    "                        X[i, :] = Z[0, :]  # 赋值\n",
    "\n",
    "    return X  # 返回数据\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 读取数据\n",
    "    df = pd.read_excel('data_ABTS_320.xlsx')\n",
    "\n",
    "    # 用Pandas工具查看数据\n",
    "    print(df.head())\n",
    "\n",
    "    # 查看数据集摘要\n",
    "    print(df.info())\n",
    "\n",
    "    # 数据描述性统计分析\n",
    "    print(df.describe())\n",
    "\n",
    "    # y变量分布直方图\n",
    "    fig = plt.figure(figsize=(8, 5))  # 设置画布大小\n",
    "    plt.rcParams['font.sans-serif'] = 'SimHei'  # 设置中文显示\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 解决保存图像是负号'-'显示为方块的问题\n",
    "    data_tmp = df['y']  # 过滤出y变量的样本\n",
    "    # 绘制直方图  bins：控制直方图中的区间个数 auto为自动填充个数  color：指定柱子的填充色\n",
    "    plt.hist(data_tmp, bins='auto', color='g')\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('数量')\n",
    "    plt.title('y变量分布直方图')\n",
    "    plt.show()\n",
    "\n",
    "    # 数据的相关性分析\n",
    "\n",
    "    #sns.heatmap(df.corr(), cmap=\"YlGnBu\", annot=True)  # 绘制热力图\n",
    "    #plt.title('相关性分析热力图')\n",
    "    #plt.show()\n",
    "\n",
    "    # 提取特征变量和标签变量\n",
    "    y = df['y']\n",
    "    X = df.drop('y', axis=1)\n",
    "\n",
    "    # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    X_train = layers.Lambda(lambda X_train: K.expand_dims(X_train, axis=-1))(X_train)  # 增加维度\n",
    "\n",
    "    print('***********************查看训练集的形状**************************')\n",
    "    print(X_train.shape)  # 查看训练集的形状\n",
    "\n",
    "    X_test = layers.Lambda(lambda X_test: K.expand_dims(X_test, axis=-1))(X_test)  # 增加维度\n",
    "    print('***********************查看测试集的形状**************************')\n",
    "    print(X_test.shape)  # 查看测试集的形状\n",
    "\n",
    "    # 参数初始化\n",
    "    N = 10  # 种群数量\n",
    "    T = 2  # 最大迭代次数\n",
    "\n",
    "    opts = {'N': N, 'T': T}\n",
    "\n",
    "    # 调用哈里斯鹰优化算法主函数\n",
    "    fmdl = jfs(X_train, y_train, X_test, y_test, opts)\n",
    "\n",
    "    if abs(fmdl[0][0]) > 0:  # 判断\n",
    "        best_units = int(abs(fmdl[0][0])) * 10 + 48  # 赋值\n",
    "    else:\n",
    "        best_units = int(abs(fmdl[0][0])) + 48  # 赋值\n",
    "\n",
    "    if abs(fmdl[0][1]) > 0:  # 判断\n",
    "        best_epochs = int(abs(fmdl[0][1])) * 10 + 60  # 赋值\n",
    "    else:\n",
    "        best_epochs = (int(abs(fmdl[0][1])) + 100)  # 赋值\n",
    "\n",
    "    print('----------------HHO哈里斯鹰优化算法优化LSTM模型-最优结果展示-----------------')\n",
    "    print(\"The best units is \" + str(abs(best_units)))\n",
    "    print(\"The best epochs is \" + str(abs(best_epochs)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 应用优化后的最优参数值构建LSTM回归模型\n",
    "    lstm = Sequential()  # 序贯模型\n",
    "    lstm.add(LSTM(units=128, return_sequences=True, input_shape=(X_train.shape[1], 1)))  # LSTM层\n",
    "    lstm.add(LSTM(units=128, return_sequences=True)\n",
    "    lstm.add(LSTM(units=32))  # LSTM层\n",
    "    lstm.add(Dense(best_units, activation='tanh'))  # 全连接层\n",
    "    lstm.add(Dense(1))  # 输出层\n",
    "    lstm.compile(loss='mean_squared_error',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['mse'])  # 编译\n",
    "    history = lstm.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=best_epochs, batch_size=16)  # 拟合\n",
    "    print('*************************输出模型摘要信息*******************************')\n",
    "    print(lstm.summary())  # 输出模型摘要信息\n",
    "\n",
    "    plot_model(lstm, to_file='model.png', show_shapes=True)  # 保存模型结构信息\n",
    "\n",
    "\n",
    "    # 定义绘图函数：损失曲线图和准确率曲线图\n",
    "    def show_history(history):\n",
    "        loss = history.history['loss']  # 获取损失\n",
    "        val_loss = history.history['val_loss']  # 测试集损失\n",
    "        epochs = range(1, len(loss) + 1)  # 迭代次数\n",
    "        plt.figure(figsize=(12, 4))  # 设置图片大小\n",
    "        plt.subplot(1, 2, 1)  # 增加子图\n",
    "        plt.plot(epochs, loss, 'r', label='Training loss')  # 绘制曲线图\n",
    "        plt.plot(epochs, val_loss, 'b', label='Test loss')  # 绘制曲线图\n",
    "        plt.title('Training and Test loss')  # 设置标题名称\n",
    "        plt.xlabel('Epochs')  # 设置x轴名称\n",
    "        plt.ylabel('Loss')  # 设置y轴名称\n",
    "        plt.legend()  # 添加图例\n",
    "        plt.show()  # 显示图片\n",
    "\n",
    "\n",
    "    show_history(history)  # 调用绘图函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48faaf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
